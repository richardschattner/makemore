{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aa15707-afd4-4b48-a8d2-bc184dda2020",
   "metadata": {},
   "source": [
    "### This notebook creates a trigram model for the generation of new names.  \n",
    "We do this as follows:\n",
    "1. Read in the names from a list of names.\n",
    "2. Generate a dictionary to get a mapping from chars to indexes and vice versa.\n",
    "3. Create the list of all trigrams of words in the name list.\n",
    "4. Create a tensor N to count the occurences of each trigram\n",
    "5. Turn this tensor into a tensor P, which counts the relative frequencies of each third char, given two previous ones.\n",
    "6. Use this tensor in order to sample new chars and generate new names.\n",
    "7. Calculate the average negative likelihood of each word in the test set.\n",
    "\n",
    "Since we used the relative frequencies of the trigrams in the training set as our probabilities to sample new chars, our model is optimal wrt. to minimizing the expected NLL among all models, which only consider trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b5fc732e-e7ed-464f-9ef0-9bc74dd3a349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#read in the names from the files\n",
    "f = open(\"names.txt\", \"r\")\n",
    "words = f.read().splitlines()\n",
    "\n",
    "#get chars and a dictionary converting chars to numbers and vice versa\n",
    "chars = sorted(list(set(\"\".join(words) )))\n",
    "stoi = {s : i+1 for i,s in enumerate(chars)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i: s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "23d21435-b715-4f7a-9ce2-a05bb746bc67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#build up the dataset of all trigrams\n",
    "x = []\n",
    "x_test = []\n",
    "split = int(len(words)*0.8)\n",
    "\n",
    "for w in words[:split]:\n",
    "    context = [0]*3\n",
    "    \n",
    "    for char in w + \".\":\n",
    "        ix = stoi[char]\n",
    "        context = context[1:] + [ix]\n",
    "        x.append(context)\n",
    "\n",
    "for w in words[split+1:]:\n",
    "    context = [0]*3\n",
    "    \n",
    "    for char in w + \".\":\n",
    "        ix = stoi[char]\n",
    "        context = context[1:] + [ix]\n",
    "        x_test.append(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "5d78fec2-2f57-4315-ad10-9f1269fb7761",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 5],\n",
       " [0, 5, 13],\n",
       " [5, 13, 13],\n",
       " [13, 13, 1],\n",
       " [13, 1, 0],\n",
       " [0, 0, 15],\n",
       " [0, 15, 12],\n",
       " [15, 12, 9],\n",
       " [12, 9, 22],\n",
       " [9, 22, 9],\n",
       " [22, 9, 1],\n",
       " [9, 1, 0],\n",
       " [0, 0, 1],\n",
       " [0, 1, 22],\n",
       " [1, 22, 1],\n",
       " [22, 1, 0],\n",
       " [0, 0, 9],\n",
       " [0, 9, 19],\n",
       " [9, 19, 1],\n",
       " [19, 1, 2]]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "985f41e5-3367-461b-809b-def67d8e494d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "#get a tensor which saves the number of occurences of each 3-gram in the dataset\n",
    "#smooth the tensor out by adding 1 to each 3-gram count\n",
    "\n",
    "N = torch.ones(27,27,27)\n",
    "\n",
    "for count in x:\n",
    "    a,b,c = count\n",
    "    N[a,b,c] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "4fbe943d-29d7-4f45-825c-1f6d3f301cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#we set all counts of three consecutive dots to zero.\n",
    "N[0,0,0]=0\n",
    "\n",
    "#in order to samle from N, we convert it into relative frequencies, by broadcasting\n",
    "P = N / N.sum(dim = 2, keepdim = True)\n",
    "\n",
    "#get rid of nan in the first row, created by division by 0\n",
    "P[0,0,0]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "e3b32580-7094-46a4-b6a8-282a4d50b6fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]])"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check that we calculated relative frequencies\n",
    "#all entries are 1, except for the ones we manually set to zero\n",
    "P.sum(dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "98eee1b2-a82c-422d-9ead-808a542a005e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an.', 'ana', 'ana', 'an.', 'ani', 'an.', 'and', 'ann', 'ani', 'ane', 'ane', 'ana', 'ann', 'ana', 'an.', 'an.', 'ani', 'an.', 'ane', 'ann', 'ant', 'anz', 'ano', 'ann', 'ann', 'an.', 'an.', 'any', 'ann', 'and', 'and', 'and', 'ann', 'ant', 'any', 'an.', 'anv', 'ana', 'and', 'ana', 'an.', 'ana', 'ana', 'and', 'anc', 'ana', 'ann', 'ana', 'ana', 'ana', 'ani', 'ana', 'and', 'ann', 'ann', 'ana', 'ann', 'ane', 'anq', 'an.', 'ani', 'any', 'an.', 'ana', 'ana', 'ane', 'and', 'ane', 'ang', 'ana', 'an.', 'ani', 'ana', 'ang', 'and', 'ani', 'ang', 'ana', 'ani', 'ann', 'an.', 'ana', 'ant', 'anh', 'ann', 'ann', 'ann', 'any', 'ann', 'ana', 'anz', 'ani', 'and', 'ang', 'ann', 'and', 'an.', 'ano', 'ann', 'ans']\n"
     ]
    }
   ],
   "source": [
    "#set seed and test sampling from the trigram frequencies.\n",
    "g = torch.Generator().manual_seed(1)\n",
    "\n",
    "ix = torch.multinomial(P[1,14,:], num_samples = 100, replacement = True , generator = g)\n",
    "\n",
    "bi = itos[1] + itos[14]\n",
    "trigrams = []\n",
    "for i in ix:\n",
    "    trigrams.append(bi + itos[i.item()])\n",
    "print(trigrams)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "fa853b72-0762-488f-b07f-a780aad80af3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ana': 21,\n",
       "         'ann': 18,\n",
       "         'an.': 15,\n",
       "         'and': 11,\n",
       "         'ani': 9,\n",
       "         'ane': 6,\n",
       "         'any': 4,\n",
       "         'ang': 4,\n",
       "         'ant': 3,\n",
       "         'anz': 2,\n",
       "         'ano': 2,\n",
       "         'anv': 1,\n",
       "         'anc': 1,\n",
       "         'anq': 1,\n",
       "         'anh': 1,\n",
       "         'ans': 1})"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "8a1c0520-d50c-4915-bef5-1717a99a33d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anne.\n",
      "de.\n",
      "jew.\n",
      "prnbff.\n",
      "guah.\n",
      "da.\n",
      "arihyaa.\n",
      "alexya.\n",
      "jedyla.\n",
      "kimitiandrissjkxpgiustoni.\n",
      "aanny.\n",
      "ne.\n",
      "agana.\n",
      "asmeyviahmiva.\n",
      "leanier.\n",
      "tapb.\n",
      "rulei.\n",
      "on.\n",
      "rosellyn.\n",
      "emmaxib.\n"
     ]
    }
   ],
   "source": [
    "#generate a few names\n",
    "\n",
    "for i in range(20):\n",
    "    out = \"\"\n",
    "    a, b = 0, 0\n",
    "    \n",
    "    while True:\n",
    "        #sample next char\n",
    "        sample = torch.multinomial(P[a,b,:], num_samples = 1, generator = g).item()\n",
    "        out += itos[sample]\n",
    "        #update previous trigrams\n",
    "        a = b\n",
    "        b = sample\n",
    "        #stop when a dot is sampled\n",
    "        if sample == 0: \n",
    "            break\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114b19c0-2276-40ce-9a2a-78ab833dc7dc",
   "metadata": {},
   "source": [
    "To compute a loss for our model, we consider the likelihood of each word of the test set.\n",
    "This is the product of the probabilities of each trigram.\n",
    "\n",
    "For example, given the name \"Anna\", \n",
    "$$ \\text{Lik}(\\text{\"Anna\"}) = p(..a)\\cdot p(.an)\\cdot p(ann)\\cdot p(nna) \\cdot p(na.) $$\n",
    "Maximizing this likelihood is equivalent to minimizing the log likelihood, which is again equivalent to maximizing the negative log likelihood.\n",
    "We compute the NLL for the entire test dataset and divide it by the number of words in the dataset to get the average NLL per word.\n",
    "The average NLL can also serve as a loss function for our model.\n",
    "\n",
    "Because our trigram model assigns to each trigram its relative frequency among the training set, it is the maximum likelihood estimator for all models considering only trigrams.\n",
    "That is to say, that among all models, which only consider trigrams, our trigram model (almost) minimizes the expected NLL of the words in the training set.  \n",
    "The model only almost minimizes the expected NLL, because we smoothed the occurence matrix N, by adding 1 to each trigram occurence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "12114345-11e7-4d63-b62f-8422fe0c3b47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.18137550354004"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#next we compute the loss of the trigram model.\n",
    "#to do so, we calculate the likelihood of each word occuring, based on the trigram frequencies.\n",
    "#as a loss function we take the NLL of each word of the test set.\n",
    "import numpy as np\n",
    "\n",
    "nll = 0\n",
    "for tri in x_test:\n",
    "    a,b,c = tri\n",
    "    nll -= np.log(P[a,b,c])\n",
    "    \n",
    "nll /= len(words)-split\n",
    "nll.item()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
